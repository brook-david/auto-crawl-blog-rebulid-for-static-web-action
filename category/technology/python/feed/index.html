<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>Python &#8211; DB`s Blog</title>
	<atom:link href="https://blog.wanderto.top/category/technology/python/feed/" rel="self" type="application/rss+xml" />
	<link>https://blog.wanderto.top</link>
	<description>随心录</description>
	<lastBuildDate>Wed, 15 May 2024 06:30:14 +0000</lastBuildDate>
	<language>zh-CN</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=6.4.3</generator>

<image>
	<url>https://blog.wanderto.top/wp-content/uploads/2024/04/cropped-logo33-32x32.png</url>
	<title>Python &#8211; DB`s Blog</title>
	<link>https://blog.wanderto.top</link>
	<width>32</width>
	<height>32</height>
</image> 
	<item>
		<title>Python爬取网站内容做静态页面转发</title>
		<link>https://blog.wanderto.top/2024/05/14/python-crawl-web-for-static-forward/</link>
					<comments>https://blog.wanderto.top/2024/05/14/python-crawl-web-for-static-forward/#respond</comments>
		
		<dc:creator><![CDATA[DB]]></dc:creator>
		<pubDate>Tue, 14 May 2024 05:43:34 +0000</pubDate>
				<category><![CDATA[Python]]></category>
		<category><![CDATA[技术]]></category>
		<category><![CDATA[GitHub]]></category>
		<category><![CDATA[K8S]]></category>
		<category><![CDATA[Nginx]]></category>
		<category><![CDATA[Scrapy]]></category>
		<guid isPermaLink="false">https://blog.wanderto.top/?p=1162</guid>

					<description><![CDATA[<p>前言 由于本网站部署的问题（云服务+本地部署），本地可能有多种原因失联。为了在这种情况下网站依然能被用户查看， [&#8230;]</p>
<p>&lt;p&gt;The post <a rel="nofollow" href="https://blog.wanderto.top/2024/05/14/python-crawl-web-for-static-forward/">Python爬取网站内容做静态页面转发</a> first appeared on <a rel="nofollow" href="https://blog.wanderto.top">DB`s Blog</a>.&lt;/p&gt;</p>
]]></description>
										<content:encoded><![CDATA[
<h2 class="wp-block-heading">前言</h2>



<pre class="wp-block-preformatted">由于本网站部署的问题（云服务+本地部署），本地可能有多种原因失联。为了在这种情况下网站依然能被用户查看，所以将网站内容爬取后静态保存，然后利用Nginx的异常状态捕捉，当服务不可用时将URI对应的静态文件返回给浏览器。</pre>



<h2 class="wp-block-heading">页面爬取</h2>



<pre class="wp-block-preformatted">以下使用了Python的<a href="https://scrapy.org/" target="_blank" rel="noopener">scrapy</a>框架，根据官网的<a href="https://docs.scrapy.org/en/latest/intro/tutorial.html" data-type="link" data-id="https://docs.scrapy.org/en/latest/intro/tutorial.html" target="_blank" rel="noopener">Scrapy Tutorial</a>提供的scrapy startproject tutorial命令可以快速构建一个爬虫项目，创建自己的爬虫文件并编写相关逻辑后执行scrapy crawl blog命令启动爬虫，爬虫文件参考如下：</pre>



<pre class="EnlighterJSRAW" data-enlighter-language="python" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">from pathlib import Path

import scrapy
import logging
from typing import TYPE_CHECKING, Any, Iterable, List, Optional, Union, cast


class QuotesSpider(scrapy.Spider):
    # 爬虫名 启动爬虫 scrapy crawl blog
    name = "blog"
    # 下载域名
    downloadHost = "xxx.xxx.com"
    # 保存目录
    saveDir = "blog"
    # 已访问列表
    accessedList = []
    # 已下载列表
    downloadedList = []
    # 爬取开始地址
    start_urls = [
        "https://xxx.xxx.com",
    ]

    def parse(self, response):
        self.log("访问pares:" + response.url)
        # 添加到已访问集合
        self._addAccess(response.url)
        # 下载当前页面
        self.download(response)

        # 资源下载
        # 图片下载
        images = response.css("img::attr(src)")
        self.log("images :" + str(images))
        for item in images:
            s_item = str(item)
            if s_item.__contains__(self.downloadHost) and not self._isDownloaded(s_item):
                yield response.follow(s_item, self.download)
        # link(css)下载
        links = response.css("link::attr(href)")
        self.log("links :" + str(links))
        for item in links:
            # 移除url后的参数
            s_item = self._removeParameter(str(item))
            if s_item.__contains__(self.downloadHost) and not self._isDownloaded(s_item):
                yield response.follow(s_item, self.download)
        # script下载
        scripts = response.css("script::attr(src)")
        self.log("scripts :" + str(scripts))
        for item in scripts:
            # 移除url后的参数
            s_item = self._removeParameter(str(item))
            if s_item.__contains__(self.downloadHost) and not self._isDownloaded(s_item):
                yield response.follow(s_item, self.download)

        # 其他页面爬取
        aList = response.css("a::attr(href)")
        self.log("")
        for a in aList:
            stra = str(a)
            if stra.__contains__(self.downloadHost):
                if self._isAccessed(stra):
                    self.log("已下载过的本站地址：" + stra)
                    continue

                self.log("可下载的本站地址："+stra)
            else:
                self.log("不可下载的地址：" + stra)
                continue
            yield response.follow(a, self.parse)

    def download(self, response):
        self._download(response.url, response.body)
        # filename = f"quotes-{page}.html"
        # Path(filename).write_bytes(response.body)

    def _download(self, url, body):
        if self._isDownloadedAndSave(url):
            return
        # https://xxx.xxx.com/xx/xx/
        url = self._switchHtml(url)
        # self.log("进行页面下载流程，下载文件："+url)

        # 获取https://后的内容
        paths = url.split("/")[2:]
        if paths[0].__eq__(self.downloadHost):
            self.log("下载页面：" + url)
        else:
            return

        # 去除host
        paths = paths[1:]

        for index, name in enumerate(paths):
            path = self._getMainDir() + "/" + "/".join(paths[0:index + 1])
            file = Path(path)
            if name.__eq__(paths[-1]):
                # self.log("file:" + name)
                if file.exists():
                    self.log("warn: file exists " + path)
                    continue
                file.write_bytes(body)
            else:
                # self.log("dir:" + name)
                if file.exists():
                    continue
                file.mkdir()

    def _getMainDir(self):
        dir = Path(self.saveDir)
        if not dir.exists():
            dir.mkdir()
        return self.saveDir

    def _switchHtml(self, url):
        # /与host结尾则为页面
        if url.endswith("/"):
            url = url + "index.html"
        elif url.endswith(self.downloadHost):
            url = url + "/index.html"
        return url

    def _isDownloaded(self, url):
        if url in self.downloadedList:
            return True
        return False

    def _isDownloadedAndSave(self, url):
        if self._isDownloaded(url):
            return True
        self.downloadedList.append(url)
        return False

    def _isAccessed(self, url):
        if url in self.accessedList:
            return True
        return False

    def _addAccess(self, url):
        if not self._isAccessed(url):
            self.accessedList.append(url)

    def _removeParameter(self, url):
        return url.split("?")[0]

    def log(self, message: Any, level: int = logging.DEBUG, **kw: Any) -> None:
        print(message)


if __name__ == '__main__':
    q = QuotesSpider();
    # q._download(q.downloadHost, b"aa")
    # print(q._isDownloadedAndSave(q.downloadHost))
    # print(q._removeParameter(q.downloadHost+"/wp-content/plugins/enlighter/cache/enlighterjs.min.css?ver=vo/Yz0k1HSy0Sr5"))
    print(q.downloadHost+"/2024/04/28/wordpress-custome-css-define-website/".split("/"))</pre>



<h2 class="wp-block-heading">提交Github仓库</h2>



<pre class="wp-block-preformatted">1.创建仓库
登录Github创建仓库，仓库名需要设置为“xxxx.github.io"，这样Github会为当前仓库创建一个Github Pages，如果命名为“用户名.github.io”更好，这样就直接拥有了一个Github域名下的主页。</pre>



<pre class="wp-block-preformatted">2.Git提交
将爬取下来的文件提交到Github上：</pre>



<pre class="EnlighterJSRAW" data-enlighter-language="bash" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">[root@iZbp1605iwejf5qgem2c7hZ ~]# cd blog
[root@iZbp1605iwejf5qgem2c7hZ blog]# git init
[root@iZbp1605iwejf5qgem2c7hZ blog]# git add .
[root@iZbp1605iwejf5qgem2c7hZ blog]# git commit -m init
[root@iZbp1605iwejf5qgem2c7hZ blog]# git remote add origin https://github.com/xxxxxx/xxxxxx.github.io.git
[root@iZbp1605iwejf5qgem2c7hZ blog]# git checkout -b main
[root@iZbp1605iwejf5qgem2c7hZ blog]# git push -u origin main</pre>



<h2 class="wp-block-heading">Ingress转发至静态地址</h2>



<pre class="wp-block-preformatted">1.配置Ingress当服务不可用时转发Github备份页面：</pre>



<pre class="EnlighterJSRAW" data-enlighter-language="generic" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group=""># ingress snippets配置:
    nginx.org/server-snippets: |
      error_page 502 = @fallback;
      location @fallback {
        proxy_pass https://xxxxxx.github.io;
      }
</pre>



<pre class="wp-block-preformatted">需要注意，snippets一旦配置有问题可能会导致整个集群网络异常！其中指定的域名必需是实时可解析的！所以不建议配置k8s service name
参考：<a href="https://blog.wanderto.top/2024/05/15/nginx-ingress-controller-snippets-configration-lead-to-cluster-network-exception/" data-type="link" data-id="https://blog.wanderto.top/wp-admin/post.php?post=1183&amp;action=edit">Nginx-Ingress-Controller Snippets配置引起集群网络异常记录</a></pre>



<pre class="wp-block-preformatted">2.需要将Nginx Ingress Controller配置到控制节点，以免工作节点异常导致不可用
2.1 查看node taints：</pre>



<pre class="EnlighterJSRAW" data-enlighter-language="bash" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">[root@iZbp1605iwejf5qgem2c7hZ blog]# kubectl describe node|grep role -n
8:                    node-role.kubernetes.io/control-plane=
18:Taints:             node-role.kubernetes.io/control-plane:NoSchedule</pre>



<pre class="wp-block-preformatted">2.2 在Nginx-Ingress-Controller Deployment配置污点容忍：</pre>



<pre class="EnlighterJSRAW" data-enlighter-language="generic" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group=""># 配置在spec.template.spec下
	tolerations:  
	- key: "node-role.kubernetes.io/control-plane"  
	  operator: "Equal"  
	  value: ""  
	  effect: "NoSchedule"</pre>



<pre class="wp-block-preformatted">配置完成！当工作节点丢失后Nginx Ingress Controller会提示502网关错误，然后触发502转发，将Github上对应URI文件返回给浏览器。不过不知道是不是Github限流，页面加载有点慢！单文件加载还行，但是一个页面通常有几十个文件，在浏览器缓存了一些文件之后速度才好一些。
</pre>
<p>&lt;p&gt;The post <a rel="nofollow" href="https://blog.wanderto.top/2024/05/14/python-crawl-web-for-static-forward/">Python爬取网站内容做静态页面转发</a> first appeared on <a rel="nofollow" href="https://blog.wanderto.top">DB`s Blog</a>.&lt;/p&gt;</p>
]]></content:encoded>
					
					<wfw:commentRss>https://blog.wanderto.top/2024/05/14/python-crawl-web-for-static-forward/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
	</channel>
</rss>
